<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 该论文提出了人工智能伦理熵的第二定律，类似于热力学第二定律，表明未经约束的AI会自发偏离目标，需要持续的对齐工作来维持稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统在缺乏持续对齐工作的情况下如何自发偏离其预期目标，为AI对齐问题提供定量理论基础。

Method: 定义了基于目标概率分布的伦理熵概念，推导了熵随时间增加的理论证明，建立了临界对齐工作边界，并通过模拟实验验证理论。

Result: 70亿参数模型在无约束情况下熵从0.32增加到1.69±1.08纳特，而施加对齐工作（γ=20.4）的系统保持稳定在0.00±0.00纳特。

Conclusion: 将AI对齐重新定义为连续热力学控制问题，为高级自主系统的稳定性和安全性提供了定量基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>
