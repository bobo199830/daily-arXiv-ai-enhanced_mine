<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: G²VLM是一个几何基础的视觉语言模型，通过将3D空间重建与空间理解相结合，解决了现有视觉语言模型在空间智能方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间智能方面表现不佳，缺乏从2D图像重建3D空间的视觉几何学习过程。

Method: G²VLM利用学习的3D视觉几何特征，通过上下文学习和交错推理直接预测3D属性并增强空间推理任务，采用统一设计在多视角图像和视频数据上进行训练。

Result: 实验结果显示G²VLM在3D重建任务上达到最先进水平，在空间理解和推理任务上表现优于或与现有方法相当。

Conclusion: 通过将语义强大的视觉语言模型与低层3D视觉任务统一，G²VLM为社区提供了强大的基准，有望解锁更多未来应用如3D场景编辑。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [2] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL是Qwen系列中最强大的视觉语言模型，在多个多模态基准测试中表现优异，支持高达256K token的交错上下文，无缝集成文本、图像和视频。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理长上下文、支持多模态输入（文本、图像、视频）的先进视觉语言模型，满足现实工作流中图像推理、智能决策和多模态代码智能的需求。

Method: 采用增强的交错MRoPE技术进行时空建模，集成DeepStack以利用多级ViT特征加强视觉语言对齐，以及基于文本的时间对齐技术用于视频处理。提供密集模型和MoE变体以适应不同的延迟-质量权衡。

Result: 在MMMU和视觉数学基准测试（如MathVista和MathVision）上表现出领先性能，纯文本理解能力超越可比文本模型，长上下文理解能力强大。

Conclusion: Qwen3-VL在密集和MoE架构下均实现优越性能，可作为图像推理、智能决策和多模态代码智能的基础引擎。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [3] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出了一种新颖的AI错误修正系统，通过少样本学习在资源受限设备上实现高效错误修正，结合服务器端基础模型训练和设备端原型分类，无需模型重训练即可修正错误。


<details>
  <summary>Details</summary>
Motivation: AI模型在日常设备中普及，但预测错误会降低用户体验。现有解决方案主要关注错误检测，缺乏高效的修正机制，特别是在资源受限设备上。

Method: 系统包含两个关键组件：(1) 服务器端管道利用知识蒸馏将基础模型的鲁棒特征表示转移到设备兼容架构；(2) 设备端机制通过原型适配实现超高效错误修正，使用原型更新而非模型重训练。

Result: 在图像分类和物体检测任务上验证系统有效性，在Food-101和Flowers-102数据集上实现超过50%的单样本错误修正率，遗忘率低于0.02%，计算开销可忽略。

Conclusion: 通过Android演示应用验证了系统在实际场景中的实用性，为资源受限设备提供了高效的AI错误修正解决方案。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [4] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: ADVLA是一种针对视觉-语言-动作模型的对抗攻击框架，通过在视觉编码器到文本特征空间的投影上直接应用扰动，实现高效、低幅度且局部稀疏的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法需要昂贵的端到端训练，且生成的扰动补丁明显可见。为了解决这些限制，需要开发更高效、更隐蔽的攻击方法。

Method: ADVLA直接在视觉编码器投影到文本特征空间的特征上应用对抗扰动，采用注意力引导使扰动集中且稀疏，并引入三种策略增强敏感性、强制稀疏性和集中扰动。

Result: 在L∞=4/255约束下，ADVLA结合Top-K掩码修改少于10%的补丁，攻击成功率接近100%。扰动集中在关键区域，在整体图像中几乎不可察觉，单步迭代仅需约0.06秒。

Conclusion: ADVLA有效削弱VLA模型的下游动作预测，避免了传统补丁攻击的高训练成本和明显扰动，在攻击VLA特征空间方面具有独特的有效性和实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [5] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN是一个专注于提升视频生成中运动真实性的后训练框架，通过光学流判别器和分布匹配正则化器，在不牺牲视觉质量的前提下显著改善运动连贯性和动态效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽然帧级保真度较高，但在运动连贯性、动态效果和真实性方面仍存在抖动、重影和不合理动态等问题，主要原因是标准去噪MSE目标缺乏对时间一致性的直接监督。

Method: 基于3步蒸馏的视频扩散模型，训练基于DiT的光学流判别器来区分真实与生成的运动，并结合分布匹配正则化器来保持视觉保真度。

Result: 在Wan2.1-T2V-1.3B上的实验显示，MoGAN显著提升了运动质量：在VBench上运动得分比50步教师模型提升+7.3%，比3步DMD模型提升+13.3%；在VideoJAM-Bench上运动得分分别提升+7.4%和+8.8%，同时保持相当或更好的美学和图像质量得分。人类研究也证实MoGAN在运动质量上更受青睐。

Conclusion: MoGAN在不牺牲视觉保真度或效率的前提下，显著提升了运动真实性，为快速高质量视频生成提供了一条实用路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [6] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 提出了一种自提示的点监督框架，通过Refine-Requery-Reinforce循环机制，仅使用稀疏点标注来适配SAM模型到遥感图像分割任务，显著提升了分割质量和领域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在遥感图像上由于领域偏移和密集标注稀缺导致的性能下降问题，探索仅使用稀疏点标注的有效适配方法。

Method: 采用Refine-Requery-Reinforce循环：从初始点生成粗伪掩码（Refine），使用自构建的框提示改进（Requery），并通过嵌入对齐减少确认偏差（Reinforce）。

Result: 在WHU、HRSID和NWPU VHR-10三个遥感图像基准数据集上，该方法持续超越预训练SAM和最近的点监督分割方法。

Conclusion: 自提示和语义对齐为遥感应用中基础分割模型的可扩展点级适配提供了高效路径。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [7] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: CaFlow是一个用于长时动作质量评估的统一框架，结合了反事实去混淆和双向时间条件流，通过因果反事实正则化和双向流模块提升长期时序建模的鲁棒性和一致性。


<details>
  <summary>Details</summary>
Motivation: 长时动作质量评估（如花样滑冰、艺术体操）面临建模长期时序动态和抵抗上下文混淆因素的挑战，现有方法依赖昂贵标注或单向时序建模，易受伪相关影响且长期表示不稳定。

Method: 提出CaFlow框架，包含因果反事实正则化（CCR）模块和双向流（BiT-Flow）模块。CCR以自监督方式解耦因果和混淆特征，通过反事实干预增强因果鲁棒性；BiT-Flow建模前向和后向动态，通过循环一致性约束生成更平滑一致的表示。

Result: 在多个长时AQA基准测试上的广泛实验表明，CaFlow实现了最先进的性能。

Conclusion: CaFlow通过因果反事实正则化和双向时序建模，有效解决了长时动作质量评估中的时序动态建模和上下文混淆问题，显著提升了评估性能。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [8] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: Multi-Crit是一个评估多模态模型作为评判者的基准，重点关注模型遵循多样化、细粒度评估标准的能力，揭示了现有模型在遵循多元标准方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型越来越多地被用作多模态评估系统的评判者，但它们遵循多样化、细粒度评估标准的能力尚未得到充分探索。

Method: 通过严格的数据整理流程构建Multi-Crit基准，收集具有多标准人工标注的挑战性响应对，并引入三个新指标来系统评估多元标准遵循性、标准切换灵活性和识别标准级偏好冲突的能力。

Result: 对25个LMM的全面分析显示：1) 专有模型在遵循多元标准方面仍存在困难；2) 开源模型在灵活遵循多样化标准方面更落后；3) 使用整体判断信号进行批评微调增强了视觉基础能力，但无法泛化到多元标准级判断。

Conclusion: Multi-Crit为构建可靠和可操控的多模态AI评估奠定了基础，揭示了当前多模态评判者的局限性。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [9] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 本文首次系统研究仅从相机轨迹（而非像素）感知视频内容的可行性，提出CamFormer对比学习框架，将相机姿态轨迹与自然语言对齐，发现相机轨迹是揭示视频内容的强信号。


<details>
  <summary>Details</summary>
Motivation: 探索仅通过相机轨迹（而非像素内容）来感知视频内容的可能性，验证"如何移动"能否揭示"在做什么"或"观察什么"。

Method: 提出对比学习框架训练CamFormer编码器，将相机姿态轨迹投影到联合嵌入空间，与自然语言对齐。

Result: 相机轨迹是揭示视频内容的强信号，CamFormer嵌入在跨模态对齐、分类和时序分析等下游任务中表现优异，且对多种相机姿态估计方法具有鲁棒性。

Conclusion: 相机轨迹是一种轻量级、鲁棒且多功能的视频内容感知模态，"如何移动"确实能揭示"在做什么"或"观察什么"。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [10] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: Canvas-to-Image是一个统一的框架，将文本提示、主题参考、空间布局、姿态约束和布局注释等多种控制信号整合到单一画布界面中，通过多任务画布训练策略，使扩散模型能够联合理解并集成异构控制信号，在身份保持和控制遵循方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代扩散模型在生成高质量多样化图像方面表现出色，但在高保真度的组合和多模态控制方面仍存在困难，特别是当用户同时指定文本提示、主题参考、空间排列、姿态约束和布局注释时。

Method: 提出将多样控制信号编码为单一复合画布图像，使模型能够直接解释以进行集成视觉空间推理；策划多任务数据集并提出多任务画布训练策略，在统一学习范式中优化扩散模型以联合理解和集成异构控制信号。

Result: 在具有挑战性的基准测试中，包括多人组合、姿态控制组合、布局约束生成和多控制生成，Canvas-to-Image在身份保持和控制遵循方面显著优于最先进的方法。

Conclusion: Canvas-to-Image通过统一框架和多任务训练策略，成功实现了对多种控制信号的集成推理，在复杂控制场景下表现出优异的性能。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出了一种简单轻量级的方法来识别大语言模型中编码特定技能的神经元，通过将神经元激活与外部标签和模型置信度等辅助指标相关联，在复杂多技能场景中揭示可解释的任务特定行为。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务中表现出卓越能力，但其内部机制仍然不透明。本文旨在开发一种方法来识别编码特定技能的神经元，以增强模型的可解释性。

Method: 基于先前通过软提示训练识别技能神经元的工作，该方法将分析扩展到涉及多个技能的复杂场景。通过将神经元激活与外部标签和模型置信度等辅助指标相关联，无需手动标记聚合即可揭示可解释的任务特定行为。

Result: 在开放文本生成和自然语言推理等任务上的实证验证表明，该方法不仅能够检测驱动已知技能的神经元，还能在BigBench的算术推理任务中揭示先前未识别的捷径。

Conclusion: 该方法提供了一种简单有效的途径来识别大语言模型中编码特定技能的神经元，增强了模型的可解释性，并能够发现模型推理过程中的潜在捷径。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [12] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 研究发现除URL外，其他元数据类型（如文档质量指标）也能加速大语言模型预训练，有效元数据的共同特征是细粒度信息编码。提出元数据追加作为提高训练效率的方法，通过可学习元令牌和掩码损失可恢复部分加速效果。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅发现URL对加速预训练有用，但未探索其他元数据类型是否具有更大潜力。本研究旨在调查更广泛的元数据类型及其对预训练效率的影响。

Method: 研究多种元数据类型，引入元数据追加方法，使用可学习元令牌配合掩码损失，并通过探针分析潜在表征来理解元数据如何影响学习过程。

Result: 发现细粒度文档质量指标等元数据也能加速预训练，元数据追加作为辅助任务可提高训练效率，可学习元令牌能恢复部分加速效果。

Conclusion: 研究结果为整合元数据以提升大语言模型预训练效率和效果提供了实用指导，强调细粒度信息编码的重要性。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [13] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 研究探讨捷克母语者对AI生成和人类创作的捷克诗歌的识别能力与审美评价，发现参与者无法有效区分AI与人类诗歌，且存在作者偏见影响审美评价。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成诗歌研究主要关注英语，本研究旨在检验在形态复杂、资源较少的斯拉夫语言（捷克语）中，AI诗歌的感知和审美评价。

Method: 通过让捷克母语者识别AI和人类创作的诗歌作者并进行审美评分，使用逻辑回归模型分析识别准确性与审美评价的关系。

Result: 参与者对作者身份的识别准确率仅为45.8%，处于随机水平；审美评价显示强烈的作者偏见，认为AI生成的诗歌评分较低，但实际AI诗歌评分与人类诗歌相当或更高。

Conclusion: AI能够以令人信服的方式生成捷克诗歌，读者的作者信念与诗歌审美评价相互关联，诗歌熟悉度或文学背景对识别准确性无影响。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [14] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra方法训练小型编排器来协调智能工具，通过强化学习优化结果、效率和用户偏好，产生的8B模型Orchestrator在Humanity's Last Exam等任务上超越GPT-5，同时成本降低2.5倍。


<details>
  <summary>Details</summary>
Motivation: 解决深度复杂问题（如Humanity's Last Exam）对大型语言模型仍具挑战且计算成本高，需要更高效智能的解决方案。

Method: 提出ToolOrchestra方法，使用强化学习训练小型编排器，协调多种智能工具，奖励机制关注结果质量、效率和用户偏好。

Result: Orchestrator在HLE上得分37.1%，超越GPT-5（35.1%）且效率提升2.5倍；在tau2-Bench和FRAMES上大幅超越GPT-5，成本仅约30%。

Conclusion: 使用轻量级编排模型组合多样化工具比现有方法更高效有效，为实用可扩展的工具增强推理系统铺平道路。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [15] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文通过系统评估大型语言模型在不同任务难度下的泛化能力，发现跨难度泛化通常有限，训练数据中需要包含不同难度级别的样本。


<details>
  <summary>Details</summary>
Motivation: 现有研究对于在简单或困难数据上训练是否能带来更好结果存在分歧，本文旨在通过更客观、大规模和细粒度的分析来解决这个问题。

Method: 使用数千个不同LLM的输出和项目反应理论对六个数据集中的示例进行难度排序，仅基于LLM能力确定难度评级，排除人类主观判断。

Result: 跨难度泛化通常有限，在简单或困难数据上训练无法在整个难度范围内实现一致的改进。

Conclusion: 训练和评估数据中需要包含不同难度级别的样本，在难度方面走捷径是有风险的。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 该研究测试了大型语言模型在8拼图任务中的规划和状态推理能力，发现即使有反馈和验证器辅助，模型仍存在内部状态表示脆弱和启发式规划能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在规划和状态推理方面的真实能力，特别是在不使用代码执行或其他工具的情况下，通过经典的8拼图任务进行精确评估。

Method: 测试了四种模型在零样本、思维链和算法思维等常见提示条件下，并采用分层纠正反馈。还使用外部移动验证器提供有效移动。

Result: 反馈提高了某些模型-提示组合的成功率，但成功运行通常较长且计算昂贵。即使有移动验证器辅助，所有模型都无法解决任何谜题。定性分析揭示了两个主要缺陷：脆弱的内部状态表示和弱的启发式规划能力。

Conclusion: 当前大型语言模型在规划方面存在显著局限性，进一步进展可能需要维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [17] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 该论文提出了一个将系统动力学和结构方程建模结合到统一数学框架中的方法，用于支持负责任AI/ML的发展，解决不同方法间基础假设差异的障碍。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决未解决问题时可能放大人类偏见，需要更丰富的因果模型来指导负责任AI/ML的发展，但不同方法的基础假设差异阻碍了这一工作。

Method: 将系统动力学和结构方程建模整合到一个共同的数学框架中，能够从分布生成系统、开发方法并比较结果。

Result: 建立了一个统一的数学框架，可以弥合系统动力学和结构方程建模之间的方法差异。

Conclusion: 该框架为数据科学和AI/ML应用提供了系统动力学的基础认识论支持，有助于推进负责任AI/ML的发展。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [18] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem是一个双流记忆框架，通过分别编码视觉分心模式和逻辑推理错误，使MLLMs能够从成功和失败经验中学习，在多模态基准测试中显著提高准确率并减少重复错误。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的记忆方法存在简洁性偏差，逐渐丢失重要领域知识，且仅记录单模态行为轨迹，无法保存视觉注意和逻辑推理如何共同促成解决方案，这与人类多模态整合的语义记忆不匹配。

Method: 引入ViLoMem双流记忆框架，构建紧凑的基于模式的记忆，分别编码视觉分心模式和逻辑推理错误，采用增长-精炼原则逐步积累和更新多模态语义知识。

Result: 在六个多模态基准测试中，ViLoMem持续提高pass@1准确率，显著减少重复的视觉和逻辑错误，消融实验证实了双流记忆和明确分心-幻觉分离的必要性。

Conclusion: ViLoMem证明了错误感知多模态记忆对终身和跨领域智能学习的重要性，通过双流记忆框架有效提升了MLLMs的学习能力。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>
